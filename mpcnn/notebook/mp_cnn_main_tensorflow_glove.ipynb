{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:48.304120Z",
     "start_time": "2017-10-10T10:00:45.734527Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_csv(\"../train.csv\",encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:48.310099Z",
     "start_time": "2017-10-10T10:00:48.306092Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "dim = 300\n",
    "random_state =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:50.039736Z",
     "start_time": "2017-10-10T10:00:48.508142Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['question1_n'] = dataset.question1.apply(lambda x :text_to_wordlist(x))\n",
    "dataset['question2_n'] = dataset.question2.apply(lambda x :text_to_wordlist(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:50.299441Z",
     "start_time": "2017-10-10T10:00:50.278440Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_n</th>\n",
       "      <th>question2_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>水稻 纹枯病 的 识别 与 防治</td>\n",
       "      <td>水稻 纹枯病 的 识别 及 防治</td>\n",
       "      <td>1</td>\n",
       "      <td>水稻 纹枯病 的 识别 与 防治</td>\n",
       "      <td>水稻 纹枯病 的 识别 及 防治</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>水稻 一生 中 有 哪些 病虫害</td>\n",
       "      <td>水稻 品种 的 抗性 影响 稻 曲 病 发生 的 轻重 一般说来 有 哪些 规律</td>\n",
       "      <td>0</td>\n",
       "      <td>水稻 一生 中 有 哪些 病虫害</td>\n",
       "      <td>水稻 品种 的 抗性 影响 稻 曲 病 发生 的 轻重 一般说来 有 哪些 规律</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>水稻 早 穗 的 表现 有 哪些</td>\n",
       "      <td>水稻 早 穗 的 原因 及 预防措施 有 哪些</td>\n",
       "      <td>0</td>\n",
       "      <td>水稻 早 穗 的 表现 有 哪些</td>\n",
       "      <td>水稻 早 穗 的 原因 及 预防措施 有 哪些</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>早春 防止 水稻 烂秧 措施 有 哪些</td>\n",
       "      <td>水稻 如何 防止 烂 种 烂秧</td>\n",
       "      <td>1</td>\n",
       "      <td>早春 防止 水稻 烂秧 措施 有 哪些</td>\n",
       "      <td>水稻 如何 防止 烂 种 烂秧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>水稻 白 叶枯病 用 什么 农药 效果 好</td>\n",
       "      <td>水稻 结实 期 穗 发芽 的 原因 有 哪些 如何 防治</td>\n",
       "      <td>0</td>\n",
       "      <td>水稻 白 叶枯病 用 什么 农药 效果 好</td>\n",
       "      <td>水稻 结实 期 穗 发芽 的 原因 有 哪些 如何 防治</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               question1                                 question2  \\\n",
       "0       水稻 纹枯病 的 识别 与 防治                          水稻 纹枯病 的 识别 及 防治   \n",
       "1       水稻 一生 中 有 哪些 病虫害  水稻 品种 的 抗性 影响 稻 曲 病 发生 的 轻重 一般说来 有 哪些 规律   \n",
       "2       水稻 早 穗 的 表现 有 哪些                   水稻 早 穗 的 原因 及 预防措施 有 哪些   \n",
       "3    早春 防止 水稻 烂秧 措施 有 哪些                           水稻 如何 防止 烂 种 烂秧   \n",
       "4  水稻 白 叶枯病 用 什么 农药 效果 好              水稻 结实 期 穗 发芽 的 原因 有 哪些 如何 防治   \n",
       "\n",
       "   is_duplicate            question1_n  \\\n",
       "0             1       水稻 纹枯病 的 识别 与 防治   \n",
       "1             0       水稻 一生 中 有 哪些 病虫害   \n",
       "2             0       水稻 早 穗 的 表现 有 哪些   \n",
       "3             1    早春 防止 水稻 烂秧 措施 有 哪些   \n",
       "4             0  水稻 白 叶枯病 用 什么 农药 效果 好   \n",
       "\n",
       "                                question2_n  \n",
       "0                          水稻 纹枯病 的 识别 及 防治  \n",
       "1  水稻 品种 的 抗性 影响 稻 曲 病 发生 的 轻重 一般说来 有 哪些 规律  \n",
       "2                   水稻 早 穗 的 原因 及 预防措施 有 哪些  \n",
       "3                           水稻 如何 防止 烂 种 烂秧  \n",
       "4              水稻 结实 期 穗 发芽 的 原因 有 哪些 如何 防治  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:05.698583Z",
     "start_time": "2017-10-10T10:00:50.301452Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:05.703528Z",
     "start_time": "2017-10-10T10:01:05.700527Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:16.149965Z",
     "start_time": "2017-10-10T10:01:05.705528Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(dataset.question1_n.tolist() + dataset.question2_n.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:24.609945Z",
     "start_time": "2017-10-10T10:01:16.150956Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['question1_seq']= tokenizer.texts_to_sequences(dataset.question1_n)\n",
    "dataset['question2_seq']= tokenizer.texts_to_sequences(dataset.question2_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:25.105894Z",
     "start_time": "2017-10-10T10:01:24.610886Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(dataset,test_size=0.2, random_state= random_state)\n",
    "test_df, val_df = train_test_split(test_df,test_size=0.5, random_state= random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25657 3207 3208\n",
      "水稻 纹枯病 的 识别 与 防治\n",
      "[1, 20, 3, 58, 31, 2]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(test_df), len(val_df))\n",
    "print(train_df.question1[0])\n",
    "print(train_df.question1_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:25.256902Z",
     "start_time": "2017-10-10T10:01:25.106807Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:25.261894Z",
     "start_time": "2017-10-10T10:01:25.257866Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1477\n"
     ]
    }
   ],
   "source": [
    "num_word = len(tokenizer.word_index)\n",
    "print(num_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:25.269888Z",
     "start_time": "2017-10-10T10:01:25.263867Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r',encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        if word not in tokenizer.word_index:\n",
    "            continue\n",
    "        try:\n",
    "            embedding = [float(val) for val in splitLine[1:]]\n",
    "            model[word] = embedding\n",
    "        except:\n",
    "            pass\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:20.083782Z",
     "start_time": "2017-10-10T10:01:25.271867Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "glove_dict = loadGloveModel(\"../../glove.6B/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:20.658004Z",
     "start_time": "2017-10-10T10:02:20.084750Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 1475\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((num_word+1,dim ),dtype='float32')\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[i] = glove_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:20.667043Z",
     "start_time": "2017-10-10T10:02:20.659036Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1478, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:21.803174Z",
     "start_time": "2017-10-10T10:02:20.669009Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "min_temp_result = None\n",
    "\n",
    "def create_filter_blockA_weight(n_grams, w_dim, num_kernel):\n",
    "    weight = tf.Variable(tf.random_normal((n_grams, w_dim, num_kernel),stddev=0.1),name='blockA_W')\n",
    "    # weight = tf.Variable(tf.ones((n_grams, w_dim, num_kernel)))\n",
    "    bias = tf.Variable(tf.zeros(num_kernel),name='blockA_b')\n",
    "    return weight , bias\n",
    "\n",
    "def create_filter_blockB_weight(n_grams, w_dim, num_kernel):\n",
    "    weight = tf.Variable(tf.random_normal((n_grams, 1, 1, num_kernel), stddev=0.1),name='blockB_W')\n",
    "    # weight = tf.Variable(tf.ones((n_grams, 1, 1, num_kernel)))\n",
    "    bias = tf.Variable(tf.zeros(num_kernel),name='blockB_b')\n",
    "    return weight,bias\n",
    "\n",
    "\n",
    "def create_horizontal_conv(input,sequence_length, kernel_weight, type, min_mask):\n",
    "    '''\n",
    "\n",
    "    :param input:  N x L x w_dim\n",
    "    :param kernel_weight: [ window_size, 1, 1, number_kernel]\n",
    "    :param type:\n",
    "    :return: N x w_dim x num_kernel\n",
    "    '''\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"horizontal_conv\"):\n",
    "        i0 = tf.constant(0)\n",
    "        num_kernel = int(kernel_weight[0].get_shape()[-1])\n",
    "        # result = tf.zeros([0,w_dim, num_kernel])\n",
    "\n",
    "        input = tf.expand_dims(input, 3)\n",
    "        output = tf.nn.conv2d(input, kernel_weight[0], [1, 1, 1, 1], 'SAME') + kernel_weight[1]  # N, height, width, out_kernel\n",
    "        output = tf.nn.relu(output)\n",
    "        if type == 'min':\n",
    "            output = min_pool_operation2d(output,min_mask )\n",
    "        elif type == 'mean':\n",
    "            output = mean_pool_operation2d(output,sequence_length)\n",
    "        elif type == 'max':\n",
    "            output = tf.reduce_max(output, axis=1)  # N, out_kernel\n",
    "        else:\n",
    "            raise Exception(\"no such type\")\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "        # result = tf.while_loop(_cond, _run_conv, loop_vars=[i0, result],shape_invariants=[i0.get_shape(), tf.TensorShape([None, w_dim, num_kernel])])\n",
    "        # return result[1]\n",
    "\n",
    "\n",
    "\n",
    "def create_vertical_conv(input, sequence_length,  kernel_weight, type, min_mask):\n",
    "    '''\n",
    "\n",
    "    :param input:\n",
    "    :param sequence_length:\n",
    "    :param kernel_weight:\n",
    "    :param type:\n",
    "    :return:  N, num_kernel\n",
    "    '''\n",
    "    # num_kernel = 3\n",
    "    # filter = tf.Variable(tf.ones((2, 4, num_kernel)))\n",
    "    with tf.name_scope(\"vertical_conv\"):\n",
    "        num_kernel = int(kernel_weight[0].get_shape()[2])\n",
    "        output = tf.nn.conv1d(input, kernel_weight[0], 1,  'SAME') + kernel_weight[1]  # None, out_width  , out_kernel\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        if type == 'min':\n",
    "            output = min_pool_operation(output,min_mask )\n",
    "        elif type == 'mean':\n",
    "            output = mean_pool_operation(output,sequence_length)\n",
    "        elif type == 'max':\n",
    "            output = tf.reduce_max(output, axis=1)  # N, out_kernel\n",
    "        else:\n",
    "            raise Exception(\"no such type\")\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_direct_pool(input,num_kernel, type):\n",
    "    '''\n",
    "\n",
    "    :param input:\n",
    "    :param num_kernel:\n",
    "    :param type:\n",
    "    :return: N, num_kernel\n",
    "    '''\n",
    "    if type == 'min':\n",
    "        output = tf.tile(tf.expand_dims(tf.reduce_min(tf.reduce_min(input, axis=1), axis=1),axis=1),[1,num_kernel])\n",
    "    elif type == 'mean':\n",
    "        output = tf.tile(tf.expand_dims(tf.reduce_mean(tf.reduce_mean(input, axis=1), axis=1),axis=1),[1,num_kernel])\n",
    "    elif type == 'max':\n",
    "        output = tf.tile(tf.expand_dims(tf.reduce_max(tf.reduce_max(input, axis=1), axis=1),axis=1),[1,num_kernel])\n",
    "    else:\n",
    "        raise Exception(\"no such type\")\n",
    "\n",
    "    return output\n",
    "\n",
    "def l2distance(input1,input2):\n",
    "    l2diff =tf.reduce_sum(tf.square(tf.subtract(input1, input2)),\n",
    "                                   axis=1)\n",
    "    l2diff = tf.clip_by_value(l2diff,0.1,1e7)\n",
    "    l2diff = tf.sqrt(l2diff)\n",
    "    return l2diff\n",
    "\n",
    "def l1distance(input1,input2):\n",
    "    l1diff = tf.square(tf.subtract(input1, input2))\n",
    "    l1diff = tf.sqrt(tf.clip_by_value(l2diff,0.1,1e7))\n",
    "    l1diff = tf.reduce_sum(l1diff, axis=1)\n",
    "    return l1diff\n",
    "\n",
    "def cosine_similarity(input1,input2):\n",
    "    n_input1 = tf.nn.l2_normalize(input1, dim=1,epsilon=1e-7)\n",
    "    n_input2 = tf.nn.l2_normalize(input2, dim=1,epsilon=1e-7)\n",
    "    cosine_sim = tf.reduce_sum(tf.multiply(n_input1, n_input2), axis=1)\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "def pairwise_distance1(input1, input2):\n",
    "    '''\n",
    "    :param input1: N, num_kernel, 1\n",
    "    :param input2: N, num_kernel, 1\n",
    "    :return:\n",
    "    '''\n",
    "    with tf.name_scope(\"pairwise_distance1\"):\n",
    "        # return tf.stack([cosine_similarity(input1,input2),l2distance(input1,input2)],axis=1)\n",
    "        return tf.concat([cosine_similarity(input1,input2),l2distance(input1,input2)],axis=1)\n",
    "\n",
    "\n",
    "def pairwise_distance2(input1, input2):\n",
    "    '''\n",
    "    :param input1: N, num_kernel, 1\n",
    "    :param input2: N, num_kernel, 1\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    with tf.name_scope(\"pairwise_distance2\"):\n",
    "        # return tf.stack([cosine_similarity(input1,input2),l2distance(input1,input2)],axis=1)\n",
    "        return tf.concat([cosine_similarity(input1,input2),l2distance(input1,input2)],axis=1)\n",
    "\n",
    "\n",
    "def get_init_min_mask_value(input_sequence):\n",
    "    value = np.zeros(shape=(input_sequence.shape[0],maxlen))\n",
    "    for i, l in enumerate(input_sequence):\n",
    "        value[i, l:] = 1e7\n",
    "    return value\n",
    "\n",
    "def min_pool_operation(tf_var, min_mask):\n",
    "    '''\n",
    "\n",
    "    :param tf_var:\n",
    "    :param min_mark:\n",
    "    :return:\n",
    "    '''\n",
    "    global min_temp_result\n",
    "    min_mask = tf.expand_dims(min_mask,axis=2)\n",
    "    # min_mask = tf.reshape(min_mask, (None,min_mask_mask.shape[0], tf_var.shape[2]))\n",
    "    temp = tf.add(tf_var, tf.cast(min_mask,tf.float32))\n",
    "    min_temp_result = temp\n",
    "    return tf.reduce_min(temp, axis=1) \n",
    "\n",
    "\n",
    "def mean_pool_operation(tf_var, input_sequence):\n",
    "    '''\n",
    "\n",
    "    :param tf_var:\n",
    "    :param min_mark:\n",
    "    :return:\n",
    "    '''\n",
    "    input_sequence = tf.reshape(input_sequence,[-1,1])\n",
    "    temp = tf.divide(tf.reduce_sum(tf_var,axis=1), tf.add(tf.cast(input_sequence,tf.float32),1e-7))\n",
    "    return temp\n",
    "\n",
    "\n",
    "def min_pool_operation2d(tf_var, min_mask):\n",
    "    '''\n",
    "\n",
    "    :param tf_var:\n",
    "    :param min_mark:\n",
    "    :return:\n",
    "    '''\n",
    "    min_mask = tf.expand_dims(tf.expand_dims(min_mask,2),3)\n",
    "    min_mask = tf.tile(min_mask,[1,1,int(tf_var.shape[2]),int(tf_var.shape[3])])\n",
    "    # min_mask = tf.reshape(min_mask, (None,min_mask_mask.shape[0], tf_var.shape[2]))\n",
    "    temp = tf.add(tf_var, tf.cast(min_mask,tf.float32))\n",
    "    return tf.reduce_min(temp, axis=1)\n",
    "\n",
    "\n",
    "def mean_pool_operation2d(tf_var, input_sequence):\n",
    "    '''\n",
    "\n",
    "    :param tf_var:\n",
    "    :param min_mark:\n",
    "    :return:\n",
    "    '''\n",
    "    input_sequence = tf.reshape(input_sequence,[-1,1,1])\n",
    "    input_sequence = tf.tile(input_sequence,[1,int(tf_var.shape[2]),int(tf_var.shape[3])])\n",
    "    temp = tf.divide(tf.reduce_sum(tf_var,axis=1), tf.add(tf.cast(input_sequence,tf.float32), 1e-7))\n",
    "    return temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MPCNN:\n",
    "    \n",
    "    \n",
    "    def __init__(self, maxlen, dim, embedding_weight):\n",
    "        \n",
    "        \n",
    "        self.input = tf.placeholder(tf.int32,(None,maxlen),name='input1')\n",
    "        self.input2 = tf.placeholder(tf.int32,(None,maxlen),name='input2')\n",
    "#         self.input = tf.placeholder(tf.float32,(None,maxlen, dim),name='input1')\n",
    "#         self.input2 = tf.placeholder(tf.float32,(None,maxlen, dim),name='input2')\n",
    "        self.seq_length1 = tf.placeholder(tf.int32,(None),name='seq_len_1')\n",
    "        self.seq_length2 = tf.placeholder(tf.int32,(None),name='seq_len_2')\n",
    "        self.min_mask1 = tf.placeholder(tf.int32, (None, maxlen),name='min_mask1')\n",
    "        self.min_mask2 = tf.placeholder(tf.int32, (None, maxlen),name='min_mask2')\n",
    "        self.num_kernel_a = 32\n",
    "        self.num_kernel_b = 32\n",
    "        self.embedding_weight = tf.Variable(embedding_weight, name=\"E_W\")\n",
    "        self.y = tf.placeholder(tf.int32, shape=(None,2),name='ans')\n",
    "        \n",
    "        \n",
    "        \n",
    "        input = tf.nn.embedding_lookup(self.embedding_weight, self.input)\n",
    "        input2 = tf.nn.embedding_lookup(self.embedding_weight, self.input2)\n",
    "#         input = self.input\n",
    "#         input2 = self.input2\n",
    "        \n",
    "        num_kernel_a = self.num_kernel_a\n",
    "        num_kernel_b = self.num_kernel_b\n",
    "        seq_length1 = self.seq_length1\n",
    "        seq_length2 = self.seq_length2\n",
    "        min_mask1 = self.min_mask1\n",
    "        min_mask2 = self.min_mask2\n",
    "        y = self.y\n",
    "        \n",
    "        w_dim = dim\n",
    "        \n",
    "        n_grams_types = list(range(1,4)) + [-1]\n",
    "        blockA_type= ['max','mean']\n",
    "        blockA_weights = {}\n",
    "        self.blockA_weights = blockA_weights\n",
    "        regularizers = []\n",
    "        for n_g in n_grams_types:\n",
    "            for type in blockA_type:\n",
    "                if n_g  == - 1:\n",
    "                    continue\n",
    "                t_w = create_filter_blockA_weight(n_g,w_dim, num_kernel_a)\n",
    "                regularizers.append(tf.nn.l2_loss(t_w[0]))\n",
    "                blockA_weights[(n_g,type)] = t_w\n",
    "\n",
    "        \n",
    "        blockA_convs = [{},{}]\n",
    "        self.blockA_convs = blockA_convs\n",
    "        for n_g in n_grams_types :\n",
    "            for type in blockA_type:\n",
    "                if n_g == -1 :\n",
    "                    blockA_convs[0][(n_g,type)] = create_direct_pool(input,num_kernel_a,type)\n",
    "                    blockA_convs[1][(n_g,type)] = create_direct_pool(input2,num_kernel_a,type)\n",
    "                else:\n",
    "                    t_w = blockA_weights[(n_g,type)]\n",
    "                    blockA_convs[0][(n_g,type)] = create_vertical_conv(input,seq_length1,t_w,type, min_mask1)\n",
    "                    blockA_convs[1][(n_g,type)] = create_vertical_conv(input2,seq_length2,t_w,type, min_mask2)\n",
    "\n",
    "\n",
    "        #---------- block B ------------------\n",
    "        blockB_type= ['max','mean']\n",
    "        blockB_weights = {}\n",
    "        self.blockA_weights = blockA_weights\n",
    "        for n_g in n_grams_types:\n",
    "            for type in blockB_type:\n",
    "                if n_g  == - 1:\n",
    "                    continue\n",
    "                t_w = create_filter_blockB_weight(n_g,w_dim, num_kernel_b)\n",
    "                regularizers.append(tf.nn.l2_loss(t_w[0]))\n",
    "                blockB_weights[(n_g,type)] = t_w\n",
    "\n",
    "\n",
    "        blockB_convs = [{},{}]\n",
    "        self.blockB_convs = blockB_convs\n",
    "        for n_g in n_grams_types :\n",
    "            for type in blockB_type:\n",
    "                if n_g == -1 :\n",
    "                    continue\n",
    "                else:\n",
    "                    t_w = blockB_weights[(n_g,type)]\n",
    "                    blockB_convs[0][(n_g,type)] = create_horizontal_conv(input,seq_length1,t_w,type,min_mask1)\n",
    "                    blockB_convs[1][(n_g,type)] = create_horizontal_conv(input2,seq_length2,t_w,type,min_mask2)\n",
    "\n",
    "\n",
    "                    \n",
    "        outputs = []\n",
    "        #------------vertical-----comparison -------------\n",
    "\n",
    "        with tf.name_scope(\"vertical_comparison\"):\n",
    "            vertical_gp1 = []\n",
    "            vertical_gp2 = []\n",
    "            for type in blockA_type:\n",
    "                for n_g1 in n_grams_types:\n",
    "                    o1 = blockA_convs[0][(n_g1, type)]\n",
    "                    for n_g2 in n_grams_types:\n",
    "                        o2 = blockA_convs[1][(n_g2, type)]\n",
    "                        print(n_g1,n_g2, type)\n",
    "                        vertical_gp1.append(o1)\n",
    "                        vertical_gp2.append(o2)\n",
    "\n",
    "            vertical_gp1 = tf.stack(vertical_gp1,axis=2)\n",
    "            vertical_gp2 = tf.stack(vertical_gp2,axis=2)\n",
    "            self.temp_gp1 = vertical_gp1\n",
    "            self.temp_gp2 = vertical_gp2\n",
    "            o = pairwise_distance1(vertical_gp1, vertical_gp2)\n",
    "            outputs.append(o)\n",
    "\n",
    "\n",
    "            vertical_gp1 = []\n",
    "            vertical_gp2 = []\n",
    "            for n_g in n_grams_types:\n",
    "                if n_g == -1:\n",
    "                    continue\n",
    "                for type in blockB_type:\n",
    "                    vertical_gp1.append(blockB_convs[0][(n_g, type)])\n",
    "                    vertical_gp2.append(blockB_convs[1][(n_g, type)])\n",
    "          \n",
    "            vertical_gp1 = tf.concat(vertical_gp1,axis=2)\n",
    "            vertical_gp2 = tf.concat(vertical_gp2,axis=2)\n",
    "            self.temp_gp1 = vertical_gp1\n",
    "            self.temp_gp2 = vertical_gp2\n",
    "            o = pairwise_distance1(vertical_gp1, vertical_gp2)\n",
    "            outputs.append(o)\n",
    " \n",
    "\n",
    "        #-----------horizontal----comparison -------------------\n",
    "        with tf.name_scope(\"horizontal_comparison\"):\n",
    "            gp1 =[]\n",
    "            gp2 =[]\n",
    "            for type in blockA_type:\n",
    "                # r1 = []\n",
    "                # r2 = []\n",
    "                for n_g1 in n_grams_types:\n",
    "                    gp1.append(blockA_convs[0][(n_g1, type)]) # N, num_kernel\n",
    "                    gp2.append(blockA_convs[1][(n_g1, type)]) # N, num_kernel\n",
    "         \n",
    "            gp1 = tf.reshape(tf.concat(gp1,axis=1),(-1,len(n_grams_types),num_kernel_a * len(blockA_type)))\n",
    "            gp2 = tf.reshape(tf.concat(gp2,axis=1),(-1, len(n_grams_types), num_kernel_a  * len(blockA_type)))\n",
    "            o = pairwise_distance2(gp1, gp2)\n",
    "            outputs.append(o) \n",
    "\n",
    "        self.outputs = outputs\n",
    "        concat_output = tf.concat(outputs,axis=1)\n",
    "        self.concat_output = concat_output\n",
    "\n",
    "#         fc_ol = layers.fully_connected(concat_output, 64)\n",
    "        \n",
    "       \n",
    "        \n",
    "        def create_fc_layer(num_node, prev_input):\n",
    "            weight = tf.Variable(tf.truncated_normal([int(prev_input.shape[1]), num_node],stddev=0.1),name='fc_W')\n",
    "            regularizers.append(tf.nn.l2_loss(weight))\n",
    "            fc_biases_1 = tf.Variable(tf.zeros([num_node]),name='fc_b')\n",
    "            output = tf.nn.elu(tf.matmul(prev_input,weight) + fc_biases_1)\n",
    "            return output\n",
    "       \n",
    "        prob = tf.placeholder_with_default(1.0, shape=())\n",
    "        self.prob = prob\n",
    "        \n",
    "        concat_output = tf.nn.dropout(concat_output, prob)\n",
    "        fc_output = create_fc_layer(64, concat_output)\n",
    "        concat_output = tf.nn.dropout(fc_output, prob)\n",
    "        output = create_fc_layer(2, fc_output)\n",
    "        \n",
    "        \n",
    "        self.output = output\n",
    "\n",
    "        self.pred = tf.nn.softmax(output,dim=1)\n",
    "\n",
    "        self.total_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "        self.loss = tf.reduce_mean(self.total_loss)\n",
    "        total_l2_loss = tf.zeros(1)\n",
    "        for r in regularizers:\n",
    "            total_l2_loss += r\n",
    "        self.loss += 1e-7 * total_l2_loss\n",
    "            \n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess.run(self.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T09:46:40.281881Z",
     "start_time": "2017-10-10T09:46:40.272884Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:21.892184Z",
     "start_time": "2017-10-10T10:02:21.804175Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:05:05.621849Z",
     "start_time": "2017-10-10T10:04:58.717536Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 max\n",
      "1 2 max\n",
      "1 3 max\n",
      "1 -1 max\n",
      "2 1 max\n",
      "2 2 max\n",
      "2 3 max\n",
      "2 -1 max\n",
      "3 1 max\n",
      "3 2 max\n",
      "3 3 max\n",
      "3 -1 max\n",
      "-1 1 max\n",
      "-1 2 max\n",
      "-1 3 max\n",
      "-1 -1 max\n",
      "1 1 mean\n",
      "1 2 mean\n",
      "1 3 mean\n",
      "1 -1 mean\n",
      "2 1 mean\n",
      "2 2 mean\n",
      "2 3 mean\n",
      "2 -1 mean\n",
      "3 1 mean\n",
      "3 2 mean\n",
      "3 3 mean\n",
      "3 -1 mean\n",
      "-1 1 mean\n",
      "-1 2 mean\n",
      "-1 3 mean\n",
      "-1 -1 mean\n"
     ]
    }
   ],
   "source": [
    "model = MPCNN(maxlen,dim,embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T16:59:22.618312Z",
     "start_time": "2017-10-01T16:59:22.536303Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T16:22:54.819588Z",
     "start_time": "2017-10-01T16:22:54.717076Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:05:05.629736Z",
     "start_time": "2017-10-10T10:05:05.622751Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_feature_X(df,maxlen):\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for q1, q2 in zip(df.question1_seq.values, df.question2_seq.values):\n",
    "        \n",
    "        x1.append(q1)\n",
    "        x2.append(q2)\n",
    "        \n",
    "#         x1.append([embedding_matrix[t] for t in q1])\n",
    "#         x2.append([embedding_matrix[t] for t in q2])\n",
    "        l1.append(len(q1))\n",
    "        l2.append(len(q2))\n",
    "    \n",
    "    return pad_sequences(x1,maxlen,padding='post'), pad_sequences(x2,maxlen,padding='post'), np.array(l1),np.array(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:46.117569Z",
     "start_time": "2017-10-10T10:09:45.949764Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from ipywidgets import FloatProgress\n",
    "import time\n",
    "from IPython.display import display\n",
    "def evaluate (self, df , is_training, batch_size, sess, dropout_prob = 0.2):\n",
    "    X = get_feature_X(df,maxlen)\n",
    "    Y = pd.get_dummies(df.is_duplicate)\n",
    "    sess = self.sess\n",
    "    start_index = 0\n",
    "    final_loss = 0\n",
    "    current_total_trained =0  \n",
    "    p_bar = FloatProgress()\n",
    "    display(p_bar)\n",
    "    start_time = time.time()\n",
    "    while start_index < X[0].shape[0]:\n",
    "        temp_x1 = X[0][start_index:start_index+batch_size]\n",
    "        temp_x2 = X[1][start_index:start_index+batch_size]\n",
    "        temp_seq_len1 = X[2][start_index:start_index+batch_size]\n",
    "        temp_seq_len2 = X[3][start_index:start_index+batch_size]\n",
    "        test_y = Y[start_index:start_index+batch_size]\n",
    "\n",
    "        feed_dict = {\n",
    "            self.min_mask1: get_init_min_mask_value(temp_seq_len1),\n",
    "            self.min_mask2: get_init_min_mask_value(temp_seq_len2),\n",
    "            self.seq_length1: temp_seq_len1,\n",
    "            self.seq_length2: temp_seq_len2,\n",
    "            self.input: temp_x1,\n",
    "            self.input2: temp_x2,\n",
    "            self.y: test_y\n",
    "        }\n",
    "        \n",
    "        if is_training:\n",
    "            feed_dict[self.prob] = 1 - dropout_prob\n",
    "        \n",
    "        current_total_trained += temp_x1.shape[0]\n",
    "\n",
    "        if is_training:\n",
    "            # the exact output you're looking for:\n",
    "            _, c =  sess.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "            final_loss += c * temp_x1.shape[0]\n",
    "            #print(\"%s/%s training loss %s\"  % (start_index, X[0].shape[0], final_loss/current_total_trained))\n",
    "#             sys.stdout.write(\"\\r%s/%s training loss %s\"  % (start_index, X[0].shape[0], c))\n",
    "#             sys.stdout.flush()\n",
    "            duration = time.time() - start_time\n",
    "            speed = duration/current_total_trained\n",
    "            eta = (X[0].shape[0]-current_total_trained)*speed\n",
    "            p_bar.value = current_total_trained/X[0].shape[0]\n",
    "            p_bar.description = \"%s/%s, eta %s sec\"%(current_total_trained, X[0].shape[0], eta)\n",
    "        else:\n",
    "            c =  sess.run(self.loss, feed_dict=feed_dict)\n",
    "            final_loss += c * temp_x1.shape[0]\n",
    "        start_index += batch_size\n",
    "        \n",
    "    final_loss = final_loss/X[0].shape[0]\n",
    "    return final_loss\n",
    "\n",
    "def gradients (self, df , batch_size, sess):\n",
    "    X = get_feature_X(df,maxlen)\n",
    "    Y = pd.get_dummies(df.is_duplicate)\n",
    "    sess = self.sess\n",
    "    start_index = 0\n",
    "    final_loss = 0\n",
    "    current_total_trained =0  \n",
    "    p_bar = FloatProgress()\n",
    "    display(p_bar)\n",
    "    start_time = time.time()\n",
    "    while start_index < X[0].shape[0]:\n",
    "        temp_x1 = X[0][start_index:start_index+batch_size]\n",
    "        temp_x2 = X[1][start_index:start_index+batch_size]\n",
    "        temp_seq_len1 = X[2][start_index:start_index+batch_size]\n",
    "        temp_seq_len2 = X[3][start_index:start_index+batch_size]\n",
    "        test_y = Y[start_index:start_index+batch_size]\n",
    "\n",
    "        feed_dict = {\n",
    "            self.min_mask1: get_init_min_mask_value(temp_seq_len1),\n",
    "            self.min_mask2: get_init_min_mask_value(temp_seq_len2),\n",
    "            self.seq_length1: temp_seq_len1,\n",
    "            self.seq_length2: temp_seq_len2,\n",
    "            self.input: temp_x1,\n",
    "            self.input2: temp_x2,\n",
    "            self.y: test_y\n",
    "        }\n",
    "        \n",
    "      \n",
    "        current_total_trained += temp_x1.shape[0]\n",
    "        \n",
    "        var_grad = tf.gradients(self.loss, [self.output])[0]\n",
    " \n",
    "        # the exact output you're looking for:\n",
    "        g =  sess.run([var_grad, self.concat_output], feed_dict=feed_dict)\n",
    "        print(\"gradient %s\"  % (g))\n",
    "#             sys.stdout.write(\"\\r%s/%s training loss %s\"  % (start_index, X[0].shape[0], c))\n",
    "#             sys.stdout.flush()\n",
    "        duration = time.time() - start_time\n",
    "        speed = duration/current_total_trained\n",
    "        eta = (X[0].shape[0]-current_total_trained)*speed\n",
    "        p_bar.value = current_total_trained/X[0].shape[0]\n",
    "        p_bar.description = \"%s/%s, eta %s sec\"%(current_total_trained, X[0].shape[0], eta)\n",
    "\n",
    "        start_index += batch_size\n",
    "        break\n",
    "        \n",
    "    final_loss = final_loss/X[0].shape[0]\n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def fit(self,train_df , val_df, epochs,dropout_prob=0.2, batch_size=64,  check_point_name=\"./default_cnn_model\"):\n",
    "\n",
    "    sess = self.sess\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables ())\n",
    "    best_epoch = 0\n",
    "    best_loss = 1e9\n",
    "    os.mkdir(check_point_name)\n",
    "#     saver.save(self.sess, check_point_name+'/model', global_step=0)\n",
    "    for i in range (epochs):\n",
    "        print(\"training epoch \",i)\n",
    "        train_loss = evaluate(self,train_df,True,batch_size, sess, dropout_prob=dropout_prob)\n",
    "        print(\"train loss:\",train_loss)\n",
    "        loss = evaluate(self,val_df,False,64,sess)\n",
    "        print(\"val loss:\", loss)\n",
    "        if loss < best_loss:\n",
    "            best_epoch = i\n",
    "            best_loss = loss\n",
    "            print(\"save best_epoch %s to %s\"%(best_epoch,check_point_name))\n",
    "            saver.save(self.sess, check_point_name+'/model', global_step=i)\n",
    "            \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:48.011793Z",
     "start_time": "2017-10-10T10:09:48.009764Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#gradients(model,train_df,32,model.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:50.513911Z",
     "start_time": "2017-10-10T10:09:50.510880Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model.sess.run(model.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:50.819297Z",
     "start_time": "2017-10-10T10:09:50.816316Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit(model, train_df, val_df,epochs=2,dropout_prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T09:50:42.170063Z",
     "start_time": "2017-10-10T09:50:42.115537Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:52.974076Z",
     "start_time": "2017-10-10T10:09:52.971041Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# net(input1[0],input2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T11:18:41.362552Z",
     "start_time": "2017-10-10T11:18:41.354529Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def tunning_model(model):\n",
    "    dropouts = [0.1,0.2,0.3,0.4,0.5]\n",
    "    for d in dropouts:\n",
    "        print(\"train with dropout %s\"%(d))\n",
    "        model.sess.run(model.init)\n",
    "        best_loss = fit(model, train_df, val_df,epochs=5,dropout_prob=d, check_point_name=\"./mpcnn_model_%s\"%(d)) \n",
    "        with open('mpcnn_val_result.txt','a') as f:\n",
    "            f.write(str({'dropout':d,'score':best_loss})+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:09:08.961718Z",
     "start_time": "2017-10-10T11:18:42.219387Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with dropout 0.1\n",
      "training epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f066fc54d324f7f998ff58fb5ef39a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-530fd056feeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtunning_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-901005f5c6ff>\u001b[0m in \u001b[0;36mtunning_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train with dropout %s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_point_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./mpcnn_model_%s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mpcnn_val_result.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-0c8041f00171>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_df, val_df, epochs, dropout_prob, batch_size, check_point_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training epoch \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-0c8041f00171>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, df, is_training, batch_size, sess, dropout_prob)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# the exact output you're looking for:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mfinal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtemp_x1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#print(\"%s/%s training loss %s\"  % (start_index, X[0].shape[0], final_loss/current_total_trained))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jlan/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jlan/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jlan/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/jlan/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jlan/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tunning_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-06T22:24:19.991058Z",
     "start_time": "2017-10-06T22:24:19.971551Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-04T21:14:31.692463Z",
     "start_time": "2017-10-04T21:14:31.687432Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T20:33:59.862215Z",
     "start_time": "2017-10-10T20:33:59.842201Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout': 0.1, 'score': array([ 0.32164857], dtype=float32)}\n",
      "\n",
      "{'dropout': 0.2, 'score': array([ 0.31134441], dtype=float32)}\n",
      "\n",
      "{'dropout': 0.3, 'score': array([ 0.31280896], dtype=float32)}\n",
      "\n",
      "{'dropout': 0.4, 'score': array([ 0.31496465], dtype=float32)}\n",
      "\n",
      "{'dropout': 0.5, 'score': array([ 0.32734838], dtype=float32)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for line in open('./mpcnn_val_result.txt','r'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "dropout 0.2 is best, as loss function = 0.31134441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:00.755281Z",
     "start_time": "2017-10-10T19:42:00.057778Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:12.529669Z",
     "start_time": "2017-10-10T19:42:11.888164Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mpcnn_model_0.2/model-2\n"
     ]
    }
   ],
   "source": [
    "saver.restore(model.sess,'./mpcnn_model_0.2/model-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:14.108162Z",
     "start_time": "2017-10-10T19:42:14.104132Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MPCNN at 0x13f577511d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-06T22:18:01.472944Z",
     "start_time": "2017-10-06T22:18:01.469442Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:54.680516Z",
     "start_time": "2017-10-10T19:42:15.761496Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(model,test_df,False,64,model.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:57.292982Z",
     "start_time": "2017-10-10T19:42:57.288953Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3132183], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
